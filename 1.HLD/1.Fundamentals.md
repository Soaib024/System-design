# Basics 

### 1. An Introduction to distributed system
### 2. Horizontal vs Vertical Scaling
### 3. Monoliths vs Microservices
### 4. Load Balancing
### 5. Sharding
### 6. Single Point of Failure
### 7. Service Discovery and Heart Beat
### 8. Capacity Planning and Estimation
### 9. Content Delivery Network

# API Design

### 1. API Design Goals
### 2. API Design in Practice

# Database Deep Dive

### 1. What are Databases
### 2. Storage and Retrieval
### 3. What are NoSQL Databases
### 4. Types of Databases
### 5. What Database should we choose

# Messaging and Event Handling

### 1. The Message Queue: Problem Statement
### 2. Asynchronous Processing: Benefits
### 3. Publisher Subscriber Models
- If there is no strong consistency guarantee to be made for transactions, an event model is good to use in microservices.

### Advantages:
- Decouples a system's services.
- Easily add subscribers and publishers without informing the other.
- Converts multiple points of failure to a single point of failure.
- Interaction logic can be moved to services/ message broker.
### Disadvantages:
- An extra layer of interaction slows services
- Cannot be used in systems requiring strong consistency of data
- Additional cost to the team for redesigning, learning, and maintaining the message queues.


### 4. Event Driven Architecture


# Consistency in Distributed Systems
### 1. What is Data Consistency
- Consistency in a distributed system refers to how up-to-date a piece of data is.

### 2. Linearizable Consistency
- Highest level of consistency
- All changes which have happened in the database before the read operation will be reflected in the read query. 
- To achieve this we use a single-threaded single server. So every read-and-write request will always be ordered. 
- **Head of line blocking** - If some request is blocking server all other request will starve
- High latency, low availability and SPOF
- High consistency and easy to debug
- Spanner, FaunaDB


### Implementation
- Single Thread
- RAFT

### 3. Eventual Consistency
- At this level of consistency, we can send stale data for a read request.
- Eventually, our systems catch up with all updates and return fresh data.
- we can process read and write requests parallelly using multiple threads and/or multiple servers.
- **Note** Eventual consistency is a very loose guarantee, and is often coupled with stronger guarantees like "read your writes".
- Amazon DynamoDB, Apache Cassandra, Couchbase, MongoDB (with configurable consistency options), Redis, Memcached.


### 4. Causal Consistency
- If a previous operation is related to the current operation, then the previous operation must be executed before the current operation.
- Causal consistency is stronger and slower than eventual consistency because operations for the same key are processed sequentially.
- It is looser and faster than the linearizable and serializable consistency level because it does not wait for all previous operations to complete.
- Multiple operation for the same key will execute on a same server and possibly same thread, server will acquire global lock on a key till operation is completed.
- If we don't have a system to sequentially order the request for same key then there will be a race condition.
- It won't work for aggregate query. example sum
- CockroachDB, YugaByte DB


### 5. Quorum
- We have multiple replicas of data
- When there's a read request, query all of the replicas, then return f(all replica response), here f could be majority voting, latest timestamp, etc
- We can get consistency guarantees based on our requirements.

```
It defines how many readers must acknowledge a read operation = R.
It also defines how many writers must acknowledge a write operation = W. 
We must have a majority to take any action
We can either have an eventually consistent system (R + W <= N) or a strongly consistent system (R + W > N)

```

### 6. Data Consistency Levels Tradeoffs

|             | Linearizable | Eventual | Causal  | Quorum       |
|-------------|--------------|----------|---------|--------------|
| Consistency | Highest      | Lowest   | Mid-way | Configurable |
| Efficiency  | Lowest       | Highest  | Mid-way | Configurable |

- We may have cache at eventual consistency and database at linearizable/serializable

### 7. Transaction Isolation Levels
- Transactions are a collection of queries that perform one unit of work. They are atomic which means either all queries in a transaction are executed or none of it is executed.
- **Begin:** Marks the start of the transaction.
- **Commit:** Marks the end of the transaction, which results in the changes persisting to the database.
- **Rollback:** Marks the end of the transaction and undoes all the changes to the database.
- **Isolation:** If two transactions are running concurrently and queries in one transaction do not affect the other transaction then the two transactions are said to be isolated from each other. 


### 7. Read Uncommitted Data
- Even uncommitted data can be read by concurrent transactions. Although it is very fast, it leads to dirty reads if transactions rolled back. 
- We only need a single entry in the database and it is overwritten whenever there is an update operation

### 8. Read Committed
- One transaction can only read committed data from other transactions.
- For each transaction there will be a local copy
- If we are making an update to a key then the older value of the key stays in the database and the newer value is kept in the local copy till the commit finally goes through. 


### 9. Repeatable Reads
- Same tuple returned between consecutive reads in a same transaction.
- If two transactions concurrently change the same key to different values, we must roll back one transaction.
- This is called Optimistic Concurrency Control since we are optimistic about our changes until they are proven incompatible. 
- We take the values that we care about but we are not changing and keep a version of them. For every key, we will store all the values that it has ever had in different transaction commits


### 10. Serializable Isolation Level
- This is the highest isolation level. At this level, all transactions are executed serially. All of the operations are executed serially or every operation is ensured to not meddle with the operations of other transactions. 
- We use this isolation level when we want to avoid Phantom Reads. 
- We use causal ordering here. If two transactions use queries for the same key then they must be ordered. Transactions that do not have any conflict can run concurrently.

# Caches Deep Dive

### Caching: Basic
- Bad eviction policy could lead to more cache miss than it.
- If cache size is very small then there could be thrashing. 
- Data consistency between cache and DB can be problem.

### Caching: Global vs Local Cache
- Global cache -> Redis with multiple nodes
- Local cache -> Server's local memory
